{"paragraphs":[{"text":"%sh echo 'export SPARK_SUBMIT_OPTIONS=\"--packages com.spotify:spark-bigquery_2.11:0.2.1\"' >> /usr/lib/zeppelin/conf/zeppelin-env.sh","dateUpdated":"2017-12-04T22:34:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":[],"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1512426888825_-1279592521","id":"20171127-011846_2147086289","dateCreated":"2017-12-04T22:34:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:559","user":"anonymous","dateFinished":"2017-12-04T22:34:56+0000","dateStarted":"2017-12-04T22:34:55+0000"},{"text":"%sh cat /usr/lib/zeppelin/conf/zeppelin-env.sh\n## %sh sed -i '/export SPARK_SUBMIT_OPTIONS/d' /usr/lib/zeppelin/conf/zeppelin-env.sh\n","dateUpdated":"2017-12-04T22:34:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nexport ZEPPELIN_INTERPRETERS=\"\\\norg.apache.zeppelin.spark.SparkInterpreter,\\\norg.apache.zeppelin.spark.PySparkInterpreter,\\\norg.apache.zeppelin.spark.SparkSqlInterpreter,\\\norg.apache.zeppelin.spark.DepInterpreter,\\\norg.apache.zeppelin.rinterpreter.RRepl,\\\norg.apache.zeppelin.rinterpreter.KnitR,\\\norg.apache.zeppelin.spark.SparkRInterpreter,\\\norg.apache.zeppelin.markdown.Markdown,\\\norg.apache.zeppelin.angular.AngularInterpreter,\\\norg.apache.zeppelin.shell.ShellInterpreter,\\\norg.apache.zeppelin.hive.HiveInterpreter,\\\norg.apache.zeppelin.python.PythonInterpreter,\\\norg.apache.zeppelin.bigquery.BigQueryInterpreter\"\nexport ZEPPELIN_PORT=8080\nexport ZEPPELIN_CONF_DIR=/etc/zeppelin/conf\nexport ZEPPELIN_LOG_DIR=/var/log/zeppelin\nexport ZEPPELIN_PID_DIR=/var/run/zeppelin\nexport ZEPPELIN_WAR_TEMPDIR=/var/run/zeppelin/webapps\nexport ZEPPELIN_NOTEBOOK_DIR=/var/lib/zeppelin/notebook\nexport ZEPPELIN_WAR_TEMPDIR=/var/lib/zeppelin/webapps\nexport MASTER=yarn-client\nexport SPARK_HOME=/usr/lib/spark\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\n\n# Remove poor default memory options.\nexport ZEPPELIN_MEM=' '\nexport ZEPPELIN_JAVA_OPTS=\"${ZEPPELIN_JAVA_OPTS} -Dspark.executor.memory= \"\nexport SPARK_SUBMIT_OPTIONS=\"--packages com.spotify:spark-bigquery_2.11:0.2.1\"\n"}]},"apps":[],"jobName":"paragraph_1512426888827_-1278823023","id":"20171127-012752_793755383","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:560"},{"text":"import org.apache.commons.io.IOUtils\nimport java.net.URL\nimport java.nio.charset.Charset\n\nimport com.google.cloud.hadoop.io.bigquery.BigQueryConfiguration\nimport com.google.cloud.hadoop.io.bigquery.BigQueryFileFormat\nimport com.google.cloud.hadoop.io.bigquery.GsonBigQueryInputFormat\nimport com.google.cloud.hadoop.io.bigquery.output.BigQueryOutputConfiguration\nimport com.google.cloud.hadoop.io.bigquery.output.IndirectBigQueryOutputFormat\nimport com.google.gson.JsonObject\nimport org.apache.hadoop.io.LongWritable\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\n\nimport com.spotify.spark.bigquery._\n\n@transient\nval conf = sc.hadoopConfiguration\n\n// Input parameters.\nval projectId = conf.get(\"fs.gs.project.id\")\nval bucket = conf.get(\"fs.gs.system.bucket\")\n","dateUpdated":"2017-12-04T22:34:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.commons.io.IOUtils\n\nimport java.net.URL\n\nimport java.nio.charset.Charset\n\nimport com.google.cloud.hadoop.io.bigquery.BigQueryConfiguration\n\nimport com.google.cloud.hadoop.io.bigquery.BigQueryFileFormat\n\nimport com.google.cloud.hadoop.io.bigquery.GsonBigQueryInputFormat\n\nimport com.google.cloud.hadoop.io.bigquery.output.BigQueryOutputConfiguration\n\nimport com.google.cloud.hadoop.io.bigquery.output.IndirectBigQueryOutputFormat\n\nimport com.google.gson.JsonObject\n\nimport org.apache.hadoop.io.LongWritable\n\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\n\nimport com.spotify.spark.bigquery._\n\nconf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, file:/etc/hive/conf.dist/hive-site.xml\n\nprojectId: String = mortgage-data-warehouse\n\nbucket: String = dataproc-8adad88a-330a-4cbd-9a69-57e1d2afdebd-us\n"}]},"apps":[],"jobName":"paragraph_1512426888828_-1280746767","id":"20170927-202627_970177482","dateCreated":"2017-12-04T22:34:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:561","user":"anonymous","dateFinished":"2017-12-04T22:35:37+0000","dateStarted":"2017-12-04T22:34:58+0000"},{"text":"\r\nval fullyQualifiedInputTableId = \"mortgage-data-warehouse:FHLMC.FHLMC_copy_rev3_47m\" //\"mortgage-data-warehouse:GNMAI.GNMAI_H\"  \r\n\r\n// Input configuration.\r\nconf.set(BigQueryConfiguration.PROJECT_ID_KEY, projectId)\r\nconf.set(BigQueryConfiguration.GCS_BUCKET_KEY, bucket)\r\nBigQueryConfiguration.configureBigQueryInput(conf, fullyQualifiedInputTableId)\r\n\r\n// Load data from BigQuery.\r\nval tableData = sc.newAPIHadoopRDD(\r\n    conf,\r\n    classOf[GsonBigQueryInputFormat],\r\n    classOf[LongWritable],\r\n    classOf[JsonObject]).cache\r\n\r\nval fileData = sc.textFile(\"gs://\" + projectId + \"/FHLMC/FHLMC_all_46m/fhlmc46m_*.csv\")\r\n\r\n// Output parameters.\r\nval outputTableId = projectId + \":FHLMC.test_output\"\r\n// Temp output bucket that is deleted upon completion of job.\r\nval outputGcsPath = (\"gs://\" + bucket + \"/hadoop/tmp/bigquery/testoutput\")\r\n\r\n// Output configuration.\r\n// Let BigQueery auto-detect output schema (set to null below).\r\nBigQueryOutputConfiguration.configure(conf,\r\n                                      outputTableId,\r\n                                      null,\r\n                                      outputGcsPath,\r\n                                      BigQueryFileFormat.NEWLINE_DELIMITED_JSON,\r\n                                      classOf[TextOutputFormat[_,_]])\r\n\r\nconf.set(\"mapreduce.job.outputformat.class\",\r\n         classOf[IndirectBigQueryOutputFormat[_,_]].getName)\r\n\r\n// Truncate the table before writing output to allow multiple runs.\r\nconf.set(BigQueryConfiguration.OUTPUT_TABLE_WRITE_DISPOSITION_KEY,\r\n         \"WRITE_TRUNCATE\")\r\n\r\n// Helper to convert JsonObjects to (loan, age) tuples.\r\ndef convertToTuple(record: JsonObject) : (String, Option[Long]) = {\r\n  val loan = record.get(\"Loan_Num_Q\").getAsString //.toLowerCase\r\n  val rate = if (record.has(\"Age_Q\")) Some(record.get(\"Age_Q\").getAsLong) else None\r\n  return (loan, rate)\r\n}\r\n\r\n// Helper to convert (word, count) tuples to JsonObjects.\r\n//import com.google.gson.JsonNull\r\ndef convertToJson(pair: (String, Option[Long])) : JsonObject = {\r\n  val loan = pair._1\r\n  val count = pair._2\r\n  val jsonObject = new JsonObject()\r\n  jsonObject.addProperty(\"loan_num\", loan)\r\n//  if(count != None) jsonObject.addProperty(\"noterate_count\", count.get) else jsonObject.add(\"noterate_count\", JsonNull.INSTANCE)\r\n  count.foreach(jsonObject.addProperty(\"noterate_count\", _))\r\n  return jsonObject\r\n}\r\n\r\n\r\nval loanCounts = tableData\r\n        .map(entry => convertToTuple(entry._2))\r\n        .repartition(30)\r\n//        .groupByKey()\r\n//        .reduceByKey(_ + _)\r\n//      .map(entry => {\r\n//            val loan = entry._2.get(\"Loan_Num_Q\").getAsString.toLowerCase\r\n//            val age  = entry._2.get(\"Age_Q\").getAsLong\r\n//            (loan, age)\r\n//          (entry._2.get(\"Loan_Num_Q\").getAsString, entry._2.get(\"Age_Q\").getAsLong)\r\n//      })\r\n//      .map(entry => entry._2.getClass)\r\n\r\n//print(tableDatatext.getClass)\r\n\r\n\r\n//loanCounts.partitions.size    \r\nloanCounts.keys.take(20) //.foreach(l => println(l))      \r\nloanCounts.values.take(20) //.foreach(l => println(l))      \r\n      \r\n\r\n//    ----------------------- Test functionality -------------------------------\r\n//val x = sc.parallelize(Array((\"1501052587\",4500L), (\"1501052581\",4500L), (\"1023599344\",13000L)))\r\n//println(x.collect().mkString(\", \"))\r\n//    (x.map(pair => (null, convertToJson(pair)))\r\n//    .saveAsNewAPIHadoopDataset(conf))\r\n\r\n\r\n\r\n\r\n// Write data back into a new BigQuery table.\r\n// IndirectBigQueryOutputFormat discards keys, so set key to null.\r\n\r\n/*\r\n(loanCounts\r\n    .map(pair => (null, convertToJson(pair)))\r\n    .saveAsNewAPIHadoopDataset(conf))\r\n*/\r\n\r\n","dateUpdated":"2017-12-04T22:34:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\nfullyQualifiedInputTableId: String = mortgage-data-warehouse:FHLMC.FHLMC_copy_rev3_47m\n\n\n\n<console>:44: error: not found: value conf\n       conf.set(BigQueryConfiguration.PROJECT_ID_KEY, projectId)\n       ^\n\n\n\n<console>:44: error: not found: value BigQueryConfiguration\n       conf.set(BigQueryConfiguration.PROJECT_ID_KEY, projectId)\n                ^\n\n\n\n<console>:44: error: not found: value projectId\n       conf.set(BigQueryConfiguration.PROJECT_ID_KEY, projectId)\n                                                      ^\n"}]},"apps":[],"jobName":"paragraph_1512426888828_-1280746767","id":"20170927-202629_634302269","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:562"},{"text":"//tableData.take(10).foreach(entry => print(entry.getClass))\n//print(tableData.getClass)\n//print(fileData.getClass)\n\nval fredmd = spark.read.option(\"header\", \"true\").csv(\"gs://\" + projectId + \"/reference/FRED-MD/*.csv\") \nval fileData = sc.textFile(\"gs://\" + projectId + \"/reference/FRED-MD/*.csv\") // \"/FHLMC/FHLMC_all_46m/fhlmc46m_*.csv\"\n\nprint(fileData.getClass)\nprint(fredmd.getClass)\n\n//fileData.toDF.printSchema\n//fredmd.printSchema\n\n\n//fileData.toDF.show\n//fredmd.show\n\n\n\n//filetest.select(\"Loan_num_q\").write.csv(\"gs://\" + projectId + \"/FHLMC\n\n/*\nval test = spark.read.json(test0).cache\ntest.printSchema\n*/\n","dateUpdated":"2017-12-04T22:34:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[]},"apps":[],"jobName":"paragraph_1512426888829_-1281131516","id":"20170927-203538_9528790","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:563"},{"text":"val rdd = sc.parallelize(\n      List( (2012,\"Tesla\",\"S\"), (1997,\"Ford\",\"E350\"), (2015,\"Chevy\",\"Volt\"))\n  )\n//  val sqlContext = new SQLContext(sc)\n\n  // this is used to implicitly convert an RDD to a DataFrame.\n  //import spark.implicits._\n  import org.apache.spark.sql.types._\n  import org.apache.spark.sql._\n\n  val dataframe = rdd.toDF()\n\n  dataframe.show\n  dataframe.map(row => row.getAs[Int](\"_1\")).show \n  \n dataframe.map(row => {\n    val row1 = row.getAs[String](1)\n    val make = if (row1.toLowerCase == \"tesla\") \"S\" else row1\n    Row(row(0),make,row(2))\n  }).collect().foreach(println)","dateUpdated":"2017-12-04T22:34:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[(Int, String, String)] = ParallelCollectionRDD[11] at parallelize at <console>:39\n\nimport org.apache.spark.sql.types._\n\nimport org.apache.spark.sql._\n\ndataframe: org.apache.spark.sql.DataFrame = [_1: int, _2: string ... 1 more field]\n+----+-----+----+\n|  _1|   _2|  _3|\n+----+-----+----+\n|2012|Tesla|   S|\n|1997| Ford|E350|\n|2015|Chevy|Volt|\n+----+-----+----+\n\n+-----+\n|value|\n+-----+\n| 2012|\n| 1997|\n| 2015|\n+-----+\n\n\n\n\n<console>:50: error: Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.\n        dataframe.map(row => {\n                     ^\n"}]},"apps":[],"jobName":"paragraph_1512426888829_-1281131516","id":"20171201-202415_64877029","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:564"},{"text":"//val data = sc.wholeTextFiles(\"HDFS_PATH\")\nval fileData2 = sc.wholeTextFiles(\"gs://\" + projectId + \"/reference/FRED-MD/*.csv\") \n\nimport spark.implicits._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\n\nimport com.google.gson.JsonObject\nimport com.google.gson.JsonParser\n\n//val files = fileData2.map { case(filename, content) => filename}.filter(filename => filename.matches(\".*-[0-9]{2}.csv$\"))\n//val files = fileData2.filter{ case(filename, content) => filename.matches(\".*-[0-9]{2}.csv$\")}\n val files = fileData2.filter{ case(filename, content) => filename.matches(\".*2017-03.csv$\")}\n                     .map   { case(filename, content) => \n                                   //val name = filename.split(\"/FRED-MD/\")(1).split(\".csv\")(0) + \"-01\" \n                                   (filename, content)\n                                   //val result = content.lines.toList.zipWithIndex.map{ case (line, index) => if (index == 0) (\"vintage,\" + line).split(\",\") else  (name + \",\" + line).split(\",\")  }\n                                   //(result(0), result(1), result.drop(2))\n                                   //result(2)\n                     }\n                     \ndef csvTobq(filename: String, data: String) = { \n\n println (filename)\n \n // your logic of processing a single file comes here\n val name = filename.split(\"/FRED-MD/\")(1).split(\".csv\")(0) + \"-01\" \n \n val result = data.lines.toList.zipWithIndex.map{ case (line, index) => if (index == 0) (\"vintage,\" + line).split(\",\") else  (name + \",\" + line).split(\",\")  }\n //val scData = sc.textFile(filename)\n //val scTable = scData.map(_.split(\",\")).toDF\n\n \n //result(0).foreach(println)\n //result.drop(2).map(x => Row.fromSeq(x)).foreach(println)\n\n val datardd1 = sc.parallelize(result) \n val datardd2 = sc.parallelize(result.take(2).drop(1)).map(x => Row.fromSeq(x)) \n val schemasize = datardd2.first().size\n val datardd3 = sc.parallelize(result.drop(2)).map(x => Row.fromSeq( x ++ (\",\" * (schemasize - x.size)).split(\",\", -1)  )) //43   \n\n val headrdd = datardd1.first().map(x => x.replace(\" \", \"_\").replace(\"&\", \"and\").replace(\":\",\"\"))\n// println(datardd3.first)\n \n val df_schema =\n  StructType(\n    headrdd.map(fieldName => StructField(fieldName, StringType, true))\n  )\n    \n\n val tabledf2 = sqlContext.createDataFrame(datardd2, df_schema)\n val tabledf3 = sqlContext.createDataFrame(datardd3, df_schema)\n\n \n  val date_conform = udf((date: String) => {\n     val datepart = date.split(\"/\")     \n     java.sql.Date.valueOf(Array(datepart(2),datepart(0),datepart(1)).mkString(\"-\")) \n  }) \n   //tabledf3_1.map(row => row.getAs[String](\"sasdate\"))\n\n   val tabledf2_1 = tabledf2.select(\n          tabledf2.columns.map {\n            case \"vintage\" => tabledf2(\"vintage\").cast(DateType).as(\"vintage\")\n            case \"sasdate\" => lit(\"tcode\").as(\"Transform\")\n            case other     => tabledf2(other).cast(IntegerType).as(other)\n          }: _*\n   )\n   \n   tabledf2_1.take(1).foreach(println)\n\n //val tabledf3_1 = tabledf3.withColumn(\"sasdate\", lit(\"ch2018/3/3\")) \n //val tabledf3_1 = tabledf3.withColumn(\"sasdate\", date_conform($\"sasdate\")) \n   val tabledf3_1 = tabledf3.select(\n          tabledf3.columns.map {\n            case \"vintage\" => tabledf3(\"vintage\").cast(DateType).as(\"vintage\")\n            case \"sasdate\" => date_conform($\"sasdate\").as(\"sasdate\")\n            case other     => tabledf3(other).cast(DoubleType).as(other)\n          }: _*\n   ).filter(col(\"sasdate\") < lit(\"2017-01-01\"))\n\n   tabledf3_1.select(\"sasdate\").take(5).foreach(println)\n   \n  //tabledf2.printSchema\n  //print(datardd1.toDF.count)\n  print(tabledf3_1.toDF.count)\n \n  //val tablerdd: org.apache.spark.rdd.RDD[Row] = tabledf2.rdd\n \n\n  tabledf2.select(\"sasdate\").take(5).foreach(println)\n  tabledf3.select(\"sasdate\").take(5).foreach(println)\n  println(\"the Data class is : %s\".format(tabledf3.select(\"sasdate\")) )\n \n  println(\"--------check---------------\")\n\n  //------------------------- outputing  data of each table -------------------------------------------------------------- \n  import com.google.api.services.bigquery.model.TableSchema\n  import com.google.api.services.bigquery.model.TableFieldSchema\n  import scala.collection.JavaConverters._ \n  \n  val bq_schema_data = headrdd.toList.map(fieldName => fieldName match {\n      case \"vintage\"|\"sasdate\" => new TableFieldSchema().setName(fieldName).setType(\"DATE\").setMode(\"NULLABLE\")\n      case _                   => new TableFieldSchema().setName(fieldName).setType(\"FLOAT\").setMode(\"NULLABLE\")\n      }\n  ).asJava //\n      \n  val bq_outputschema_data = new TableSchema().setFields(bq_schema_data)\n   //println(bq_schema_data)\n\n  val outputTableId = projectId + \":FREDMD.FREDMDdata\" + name.replace(\"-\", \"\")\n  // Temp output bucket that is deleted upon completion of job.\n  val outputGcsPath = (\"gs://\" + bucket + \"/hadoop/tmp/bigquery/testoutput/\" + name)\n\n  // Output configuration.\n  // Let BigQueery auto-detect output schema (set to null below).\n  BigQueryOutputConfiguration.configure(conf,\n                                      outputTableId,\n                                      bq_outputschema_data, //null,   \n                                      outputGcsPath,\n                                      BigQueryFileFormat.NEWLINE_DELIMITED_JSON,\n                                      classOf[TextOutputFormat[_,_]])\n\n  conf.set(\"mapreduce.job.outputformat.class\",\n         classOf[IndirectBigQueryOutputFormat[_,_]].getName)\n\n  // Truncate the table before writing output to allow multiple runs.\n  conf.set(BigQueryConfiguration.OUTPUT_TABLE_WRITE_DISPOSITION_KEY,\n         \"WRITE_TRUNCATE\")\n\n\n  // save rdd of single file processed data to hdfs comes here\n  //tabledf3_1.printSchema\n  tabledf3_1.toJSON.rdd.map(pair => (null, new JsonParser().parse(pair).getAsJsonObject)).saveAsNewAPIHadoopDataset(conf)\n  \n  //------------------------- outputing  tcode of each table --------------------------------------------------------------\n  val bq_schema_code = tabledf2_1.columns.toList.map(fieldName => fieldName match {\n      case \"vintage\"   => new TableFieldSchema().setName(fieldName).setType(\"DATE\").setMode(\"NULLABLE\")\n      case \"Transform\" => new TableFieldSchema().setName(fieldName).setType(\"STRING\").setMode(\"NULLABLE\")\n      case _           => new TableFieldSchema().setName(fieldName).setType(\"INTEGER\").setMode(\"NULLABLE\")\n      }\n  ).asJava //\n      \n  val bq_outputschema_code = new TableSchema().setFields(bq_schema_code)\n  //println(bq_schema_code)\n  \n  val outputTableId_code = projectId + \":FREDMD.FREDMDtcode\" + name.replace(\"-\", \"\")\n  // Temp output bucket that is deleted upon completion of job.\n  val outputGcsPath_code = (\"gs://\" + bucket + \"/hadoop/tmp/bigquery/testoutput/tcode\" + name)\n\n  // Output configuration.\n  // Let BigQueery auto-detect output schema (set to null below).\n  BigQueryOutputConfiguration.configure(conf,\n                                      outputTableId_code,\n                                      bq_outputschema_code, //null,   \n                                      outputGcsPath_code,\n                                      BigQueryFileFormat.NEWLINE_DELIMITED_JSON,\n                                      classOf[TextOutputFormat[_,_]])\n\n  conf.set(\"mapreduce.job.outputformat.class\",\n         classOf[IndirectBigQueryOutputFormat[_,_]].getName)\n\n  // Truncate the table before writing output to allow multiple runs.\n  conf.set(BigQueryConfiguration.OUTPUT_TABLE_WRITE_DISPOSITION_KEY,\n         \"WRITE_TRUNCATE\")\n\n\n  // save rdd of single file processed data to hdfs comes here\n  //tabledf2_1.printSchema\n  tabledf2_1.toJSON.rdd.map(pair => (null, new JsonParser().parse(pair).getAsJsonObject)).saveAsNewAPIHadoopDataset(conf)\n\n  \n  \n}\n \nfiles.collect.foreach{ case (filename, content) => {\n    csvTobq(filename, content)\n}}\n\n\n//val filedata = files.map{case (result1, result2, result3) => result3 }\n                \n//filedata.collect.foreach( x => println(x.size)) \n  \n\n","dateUpdated":"2017-12-04T22:43:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\nfileData2: org.apache.spark.rdd.RDD[(String, String)] = gs://mortgage-data-warehouse/reference/FRED-MD/*.csv MapPartitionsRDD[89] at wholeTextFiles at <console>:79\n\nimport spark.implicits._\n\nimport org.apache.spark.sql.types._\n\nimport org.apache.spark.sql._\n\nimport com.google.gson.JsonObject\n\nimport com.google.gson.JsonParser\n\nfiles: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[91] at map at <console>:92\n\ncsvTobq: (filename: String, data: String)Unit\ngs://mortgage-data-warehouse/reference/FRED-MD/2017-03.csv\n[2017-03-01,tcode,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,2,2,2,5,5,2,2,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,1,2,1,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,2,6,6,5,6,6,7,6,6,6,2,5,5,2,5,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,2,6,6,6,6,1]\n[1959-01-01]\n[1959-02-01]\n[1959-03-01]\n[1959-04-01]\n[1959-05-01]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 36.0 failed 4 times, most recent failure: Lost task 1.3 in stage 36.0 (TID 51, model-6-w-2.c.mortgage-data-warehouse.internal, executor 16): ExecutorLostFailure (executor 16 exited caused by one of the running tasks) Reason: Container marked as failed: container_1512426678411_0001_01_000017 on host: model-6-w-2.c.mortgage-data-warehouse.internal. Exit status: 50. Diagnostics: Exception from container-launch.\nContainer id: container_1512426678411_0001_01_000017\nExit code: 50\nStack trace: ExitCodeException exitCode=50:\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:972)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:869)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170)\n\tat org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nContainer exited with a non-zero exit code 50\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2430)\n  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2429)\n  at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n  at org.apache.spark.sql.Dataset.count(Dataset.scala:2429)\n  at $$$$68fcb2421412a821f22fc798f81019bf$$$csvTobq(<console>:156)\n  at $$$$73dee2a55e7090b474788d5fb4a3a95$$$$$anonfun$1.apply(<console>:101)\n  at $$$$73dee2a55e7090b474788d5fb4a3a95$$$$$anonfun$1.apply(<console>:100)\n  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n  ... 72 elided\n"}]},"apps":[],"jobName":"paragraph_1512426888829_-1281131516","id":"20171109-165314_1401698027","dateCreated":"2017-12-04T22:34:48+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:565","user":"anonymous","dateFinished":"2017-12-04T22:44:35+0000","dateStarted":"2017-12-04T22:43:59+0000"},{"text":"val file = files.take(1).map(x=>x._1)\nval filedata = files.take(1).map(x=>x._2)\nval filename = file(0)\nval data = filedata(0)\n\n val name = filename.split(\"/FRED-MD/\")(1).split(\".csv\")(0) + \"-01\" \n \n\n val result = data.lines.toList.zipWithIndex.map{ case (line, index) => if (index == 0) (\"vintage,\" + line).split(\",\") else  (name + \",\" + line ).split(\",\", -1)  }\n //result(0).foreach(println)\n //result.drop(2).map(x => Row.fromSeq(x)).foreach(println)\n\n val datardd1 = sc.parallelize(result) \n val datardd2 = sc.parallelize(result.take(2).drop(1)).map(x => Row.fromSeq(x))  \n \n val schemasize = datardd2.first().size\n  \n val datardd3 = sc.parallelize(result.drop(2)).map(x => Row.fromSeq( x ++ (\",\" * (schemasize - x.size)).split(\",\", -1)  )) //43   \n                                              //.map( case Row(key:Int, value: String) => Row(key:Int, value: String))          \n/*                                              .map{ case Row(key: Int, value: String) => key match {\n                                                       //case 1 => val datepart = value.split(\"/\"); Row(1, Array(datepart(2),datepart(0),datepart(1)).mkString(\"-\"))\n                                                       case _ => Row(key, value)    \n                                                       \n                                                   }\n                                                }\n*/     \n val headrdd = datardd1.first().map(x => x.replace(\" \", \"_\").replace(\"&\", \"and\").replace(\":\",\"\"))\n \n datardd2.first().size\n //datardd3.take(1)\n datardd3.first().size\n\n println(\"...............\")\n \n println(datardd3.count)\n datardd3.first()\n //datardd2.first().foreach(println)\n\n// val df_schema0 =\n//  StructType(\n//    headrdd.map(fieldName => StructField(fieldName, StringType, true)))\n\n val df_schema =\n  StructType(\n    headrdd.map(fieldName => fieldName match {\n        case \"vintage\" => StructField(fieldName, StringType, true)\n        case _         => StructField(fieldName, StringType, true)\n    }                 \n    )\n  )\n\n        \n    \n\n val tabledf2 = sqlContext.createDataFrame(datardd3, df_schema)\n\n //tabledf2.show\n tabledf2.select(\"sasdate\").take(10)\n \n\n\n\n","dateUpdated":"2017-12-04T22:34:48+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512426888830_-1279977270","id":"20171122-214535_1236164769","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:566"},{"text":"sqlContext.setBigQueryProjectId(projectId)\nsqlContext.setBigQueryGcsBucket(bucket)\nval sampletable = sqlContext.bigQueryTable(\"mortgage-data-warehouse:GNMAI.GNMAI_H\")   //(\"bigquery-public-data:samples.shakespeare\")\nsampletable.take(3)\nsampletable.saveAsBigQueryTable(\"mortgage-data-warehouse:GNMAI.gnmatest\")  //save doesnt' work\n","dateUpdated":"2017-12-04T22:34:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":147.5,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\nsampletable: org.apache.spark.sql.DataFrame = [Record_Type_Q: string, Pool_ID_Q: string ... 386 more fields]\nres97: Array[org.apache.spark.sql.Row] = Array([L,068532,3162,V,null,null,N,Y,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2500000.0,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,44819.0,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,1,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,nul...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:525)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:211)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:194)\n  at com.databricks.spark.avro.package$AvroDataFrameWriter$$anonfun$avro$1.apply(package.scala:26)\n  at com.databricks.spark.avro.package$AvroDataFrameWriter$$anonfun$avro$1.apply(package.scala:26)\n  at com.spotify.spark.bigquery.BigQueryDataFrame.saveAsBigQueryTable(BigQueryDataFrame.scala:54)\n  at com.spotify.spark.bigquery.BigQueryDataFrame.saveAsBigQueryTable(BigQueryDataFrame.scala:67)\n  ... 132 elided\nCaused by: java.io.IOException: Failed to parse JSON: Unexpected token; Parser terminated before end of string\n  at com.google.cloud.hadoop.io.bigquery.BigQueryUtils.waitForJobCompletion(BigQueryUtils.java:95)\n  at com.google.cloud.hadoop.io.bigquery.BigQueryHelper.importFromGcs(BigQueryHelper.java:164)\n  at com.google.cloud.hadoop.io.bigquery.output.IndirectBigQueryOutputCommitter.commitJob(IndirectBigQueryOutputCommitter.java:57)\n  at org.apache.spark.sql.execution.datasources.BaseWriterContainer.commitJob(WriterContainer.scala:222)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:144)\n  ... 154 more\n"}]},"apps":[],"jobName":"paragraph_1512426888830_-1279977270","id":"20171127-015042_758610098","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:567"},{"text":"import spark.implicits._\r\nimport org.apache.spark.sql.types._\r\nimport org.apache.spark.sql._\r\nimport org.apache.spark.sql.Row;\r\nimport com.spotify.spark.bigquery._\r\n\r\n\r\nval csvData = \"\"\"|userid,organizationid,userfirstname,usermiddlename,userlastname,usertitle,createdate\r\n|1,1,user1,m1,l1,mr,1999-08-01\r\n|2,2,user2,m2,l2,mr,1999-09-01\r\n|3,3,user3,m3,l3,mr,1999-02-01\r\n|\"\"\".stripMargin\r\nval rdd = sc.parallelize(csvData.lines.toList)\r\n\r\nval rddheader = rdd.first()\r\n\r\nval rdd2 = rdd.filter(x => !x.contains(\"id\"))\r\n\r\n//val rdd3 = rdd.mapPartitionsWithIndex { (idx, iter) => if (idx == 1) iter.drop(1) else iter }\r\n\r\nval row_rdd = rdd2.map(x => \"tt\" +: x.split(',')).map(x => Row.fromSeq(x))\r\n\r\nval df_schema =\r\n  StructType(\r\n    (\"ttype\" +: rdd.first.split(',')).map(fieldName => fieldName match {\r\n      case \"createdate\" => StructField(fieldName, StringType, true)\r\n      case     _  => StructField(fieldName, StringType, true)\r\n      \r\n    }\r\n  )\r\n )     \r\n\r\n\r\n println(\"-----------------check ----------------------\")\r\n  import com.google.api.services.bigquery.model.TableSchema\r\n  import com.google.api.services.bigquery.model.TableFieldSchema\r\n\r\n   val bq_schema = rdd.first.split(',').toList  //.map{fieldName => new TableFieldSchema().setName(fieldName).setType(\"String\").setMode(\"NULLABLE\")} //\r\n\r\n\r\n\r\n\r\nvar df = sqlContext.createDataFrame(row_rdd, df_schema)\r\ndf.printSchema\r\ndf.take(3)\r\n\r\ndf.columns\r\n\r\n\r\ndf.selectExpr(\"(usertitle, userlastname) as complex\", \"*\").show\r\n\r\n\r\nval dft = df.withColumn(\"userid\", df.col(\"userid\").cast(FloatType)) //.withColumn(\"createdate\", df.col(\"createdate\").cast(\"Date\"))\r\ndft.printSchema\r\n\r\n//dft.saveAsBigQueryTable(\"mortgage-data-warehouse:FREDMD.test1\")\r\n\r\n\r\n\r\ndft.toJSON.show(false)\r\n\r\nval df2 = df.withColumn(\"createdate2\", 'createdate.cast(\"Date\"))   //.select('createdate2 as 'cdate, 'userid)\r\n\r\n\r\n\r\ndf2.describe().show()\r\n\r\nimport org.apache.spark.sql.functions.{initcap}\r\ndf2.select(initcap(col(\"userfirstname\"))).show(2, false)\r\n\r\n\r\nval filt = col(\"createdate2\") > java.sql.Date.valueOf(\"1999-08-01\")\r\n\r\ndf2.where(filt).show\r\n\r\n\r\ndf2.printSchema\r\n\r\n/*\r\nval df3 = df.select(\r\n   df.columns.map {\r\n     case \"createdate\" => df(\"createdate\").cast(DateType).as(\"createdate\")\r\n     case \"usertitle\" => functions.upper(df(\"usertitle\")).as(\"usertitle\")\r\n     case other         => df(other)\r\n   }: _*\r\n)\r\n\r\ndf3.printSchema\r\n\r\ndf3.show\r\n*/\r\n\r\n\r\n\r\n","dateUpdated":"2017-12-04T22:34:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\nimport spark.implicits._\n\nimport org.apache.spark.sql.types._\n\nimport org.apache.spark.sql._\n\nimport org.apache.spark.sql.Row\n\nimport com.spotify.spark.bigquery._\n\n\n\n\n\n\ncsvData: String =\n\"userid,organizationid,userfirstname,usermiddlename,userlastname,usertitle,createdate\n1,1,user1,m1,l1,mr,1999-08-01\n2,2,user2,m2,l2,mr,1999-09-01\n3,3,user3,m3,l3,mr,1999-02-01\n\"\n\nrdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[420] at parallelize at <console>:122\n\nrddheader: String = userid,organizationid,userfirstname,usermiddlename,userlastname,usertitle,createdate\n\nrdd2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[421] at filter at <console>:124\n\nrow_rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[423] at map at <console>:126\n\ndf_schema: org.apache.spark.sql.types.StructType = StructType(StructField(ttype,StringType,true), StructField(userid,StringType,true), StructField(organizationid,StringType,true), StructField(userfirstname,StringType,true), StructField(usermiddlename,StringType,true), StructField(userlastname,StringType,true), StructField(usertitle,StringType,true), StructField(createdate,StringType,true))\n-----------------check ----------------------\n\nimport com.google.api.services.bigquery.model.TableSchema\n\nimport com.google.api.services.bigquery.model.TableFieldSchema\n\nbq_schema: List[String] = List(userid, organizationid, userfirstname, usermiddlename, userlastname, usertitle, createdate)\n\ndf: org.apache.spark.sql.DataFrame = [ttype: string, userid: string ... 6 more fields]\nroot\n |-- ttype: string (nullable = true)\n |-- userid: string (nullable = true)\n |-- organizationid: string (nullable = true)\n |-- userfirstname: string (nullable = true)\n |-- usermiddlename: string (nullable = true)\n |-- userlastname: string (nullable = true)\n |-- usertitle: string (nullable = true)\n |-- createdate: string (nullable = true)\n\n\nres22: Array[org.apache.spark.sql.Row] = Array([tt,1,1,user1,m1,l1,mr,1999-08-01], [tt,2,2,user2,m2,l2,mr,1999-09-01], [tt,3,3,user3,m3,l3,mr,1999-02-01])\n\nres23: Array[String] = Array(ttype, userid, organizationid, userfirstname, usermiddlename, userlastname, usertitle, createdate)\n+-------+-----+------+--------------+-------------+--------------+------------+---------+----------+\n|complex|ttype|userid|organizationid|userfirstname|usermiddlename|userlastname|usertitle|createdate|\n+-------+-----+------+--------------+-------------+--------------+------------+---------+----------+\n|[mr,l1]|   tt|     1|             1|        user1|            m1|          l1|       mr|1999-08-01|\n|[mr,l2]|   tt|     2|             2|        user2|            m2|          l2|       mr|1999-09-01|\n|[mr,l3]|   tt|     3|             3|        user3|            m3|          l3|       mr|1999-02-01|\n+-------+-----+------+--------------+-------------+--------------+------------+---------+----------+\n\n\ndft: org.apache.spark.sql.DataFrame = [ttype: string, userid: float ... 6 more fields]\nroot\n |-- ttype: string (nullable = true)\n |-- userid: float (nullable = true)\n |-- organizationid: string (nullable = true)\n |-- userfirstname: string (nullable = true)\n |-- usermiddlename: string (nullable = true)\n |-- userlastname: string (nullable = true)\n |-- usertitle: string (nullable = true)\n |-- createdate: string (nullable = true)\n\n\n\n\n<console>:121: error: not found: value df3\n       df3.toJSON.show(false)\n       ^\n"}]},"apps":[],"jobName":"paragraph_1512426888831_-1280362018","id":"20171128-023141_921077054","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:568"},{"text":" println(\"-----------------check ----------------------\")\n  import com.google.api.services.bigquery.model.TableSchema\n  import com.google.api.services.bigquery.model.TableFieldSchema\n\n  //fieldSchema.add(new TableFieldSchema().setName(\"username\").setType(\"STRING\").setMode(\"NULLABLE\"));\n\n//  val fieldschema = List[TableFieldSchema](3)\n//  val schema = new TableSchema()\n//  schema.setFields(fieldschema)\n    \n   val bq_schema = rdd.first.split(',').toList.map{fieldName => new TableFieldSchema().setName(fieldName).setType(\"String\").setMode(\"NULLABLE\")} //\n","dateUpdated":"2017-12-04T22:34:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"-----------------check ----------------------\n\nimport com.google.api.services.bigquery.model.TableSchema\n\nimport com.google.api.services.bigquery.model.TableFieldSchema\n\nbq_schema: List[com.google.api.services.bigquery.model.TableFieldSchema] = List({mode=NULLABLE, name=userid, type=String}, {mode=NULLABLE, name=organizationid, type=String}, {mode=NULLABLE, name=userfirstname, type=String}, {mode=NULLABLE, name=usermiddlename, type=String}, {mode=NULLABLE, name=userlastname, type=String}, {mode=NULLABLE, name=usertitle, type=String}, {mode=NULLABLE, name=createdate, type=String})\n"}]},"apps":[],"jobName":"paragraph_1512426888831_-1280362018","id":"20171202-015353_1077894360","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:569"},{"text":"","dateUpdated":"2017-12-04T22:34:48+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512426888832_-1196102009","id":"20171202-015353_293616641","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:570"},{"text":"val outputTableId = projectId + \":FHLMC.test_json\"\r\n// Temp output bucket that is deleted upon completion of job.\r\nval outputGcsPath = (\"gs://\" + bucket + \"/hadoop/tmp/bigquery/testoutput\")\r\n\r\n// Output configuration.\r\n// Let BigQueery auto-detect output schema (set to null below).\r\nBigQueryOutputConfiguration.configure(conf,\r\n                                      outputTableId,\r\n                                      null,\r\n                                      outputGcsPath,\r\n                                      BigQueryFileFormat.NEWLINE_DELIMITED_JSON,\r\n                                      classOf[TextOutputFormat[_,_]])\r\n\r\nconf.set(\"mapreduce.job.outputformat.class\",\r\n         classOf[IndirectBigQueryOutputFormat[_,_]].getName)\r\n\r\n// Truncate the table before writing output to allow multiple runs.\r\nconf.set(BigQueryConfiguration.OUTPUT_TABLE_WRITE_DISPOSITION_KEY,\r\n         \"WRITE_TRUNCATE\")\r\n\r\n//val testout = df.select(\"ttype\",\"userid\",\"organizationid\",\"userfirstname\",\"usermiddlename\",\"userlastname\",\"usertitle\",\"createdate\").toJSON.rdd\r\nval testout = df.toJSON.rdd\r\ndf.printSchema\r\ndf.show\r\n\r\n\r\nimport com.google.gson.JsonObject\r\nimport com.google.gson.JsonParser\r\n\r\nval jsonObj = new JsonObject()\r\nval parser = new JsonParser()\r\n\r\n//val json = sc.parallelize(Array((\"1501052587\",4500L)\r\n\r\nval ss = sc.parallelize(Array((\"{\\\"a\\\": \\\"A\\\"}\")))\r\nval pss = parser.parse(\"{\\\"a\\\": \\\"A\\\"}\").getAsJsonObject\r\npss.get(\"a\")\r\n\r\nval jss = ss.toDF.toJSON.rdd\r\n\r\n\r\n//jss.map(pair => (null, new JsonParser().parse(pair).getAsJsonObject)).saveAsNewAPIHadoopDataset(conf)\r\n\r\n//.getAsJsonObject()\r\n\r\n//    ----------------------- Test functionality -------------------------------\r\n//val x = sc.parallelize(Array((\"t_type\":\"tt\",\"userid\":\"1\"), (\"t_type\":\"tt\",\"userid\":\"2\"), (\"t_type\":\"tt\",\"userid\":\"1\")))\r\nval x = sc.parallelize(Array((\"1501052587\",4500L), (\"1501052581\",4500L), (\"1023599344\",13000L)))\r\nval y = sc.parallelize(Array((\"1501052587\",Some(4500L)), (\"1501052581\",Some(4500L)), (\"1023599344\",Some(13000L))))\r\nprintln(x.collect().mkString(\", \"))\r\n\r\nval jx = x.toDF(\"userid\", \"balance\").toJSON.rdd\r\n//val jx = x.toDF().toJSON.rdd\r\n\r\n//jx.getClass\r\n//testout.getClass\r\njx.toDF.show(false)\r\ntestout.toDF.show(false)\r\n\r\n\r\n//jx.map(pair => (null, new JsonParser().parse(pair).getAsJsonObject)).saveAsNewAPIHadoopDataset(conf)\r\n\r\ntestout.map(pair => (null, new JsonParser().parse(pair).getAsJsonObject)).saveAsNewAPIHadoopDataset(conf)\r\n\r\n//val jxx = jx.map(x => println(x.getClass))\r\n//    (x.map(pair => (null, convertToJson(pair)))\r\n//    .saveAsNewAPIHadoopDataset(conf))\r\n\r\n\r\n//x.map(pair => convertToJson(pair)).\r\n\r\n//(null, json).toList\r\n//sc.parallelize((null, json)) //.saveAsNewAPIHadoopDataset(conf)\r\n\r\n//testout.map(x => new JsonParser().parse(x).getAsJsonObject())\r\n\r\n//    testout.map(row => (null, new JsonParser().parse(row).getAsJsonObject()))\r\n//    .saveAsNewAPIHadoopDataset(conf)\r\n\r\n/*\r\ndf.selectExpr(\"(usertitle, organizationid, userfirstname) as mystru\")\r\n  .select(to_json(col(\"mystru\"))).show(false)\r\n*/\r\n\r\n","dateUpdated":"2017-12-04T22:34:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\noutputTableId: String = mortgage-data-warehouse:FHLMC.test_json\n\noutputGcsPath: String = gs://dataproc-8adad88a-330a-4cbd-9a69-57e1d2afdebd-us/hadoop/tmp/bigquery/testoutput\n\ntestout: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[973] at rdd at <console>:188\nroot\n |-- ttype: string (nullable = true)\n |-- userid: string (nullable = true)\n |-- organizationid: string (nullable = true)\n |-- userfirstname: string (nullable = true)\n |-- usermiddlename: string (nullable = true)\n |-- userlastname: string (nullable = true)\n |-- usertitle: string (nullable = true)\n |-- createdate: string (nullable = true)\n\n+-----+------+--------------+-------------+--------------+------------+---------+----------+\n|ttype|userid|organizationid|userfirstname|usermiddlename|userlastname|usertitle|createdate|\n+-----+------+--------------+-------------+--------------+------------+---------+----------+\n|   tt|     1|             1|        user1|            m1|          l1|       mr|1999-08-01|\n|   tt|     2|             2|        user2|            m2|          l2|       mr|1999-09-01|\n|   tt|     3|             3|        user3|            m3|          l3|       mr|1999-02-01|\n+-----+------+--------------+-------------+--------------+------------+---------+----------+\n\n\nimport com.google.gson.JsonObject\n\nimport com.google.gson.JsonParser\n\njsonObj: com.google.gson.JsonObject = {}\n\nparser: com.google.gson.JsonParser = com.google.gson.JsonParser@26dc2001\n\nss: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[976] at parallelize at <console>:174\n\npss: com.google.gson.JsonObject = {\"a\":\"A\"}\n\nres419: com.google.gson.JsonElement = \"A\"\n\njss: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[983] at rdd at <console>:176\n\nx: org.apache.spark.rdd.RDD[(String, Long)] = ParallelCollectionRDD[984] at parallelize at <console>:174\n\ny: org.apache.spark.rdd.RDD[(String, Some[Long])] = ParallelCollectionRDD[985] at parallelize at <console>:174\n(1501052587,4500), (1501052581,4500), (1023599344,13000)\n\njx: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[993] at rdd at <console>:176\n+---------------------------------------+\n|value                                  |\n+---------------------------------------+\n|{\"userid\":\"1501052587\",\"balance\":4500} |\n|{\"userid\":\"1501052581\",\"balance\":4500} |\n|{\"userid\":\"1023599344\",\"balance\":13000}|\n+---------------------------------------+\n\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|value                                                                                                                                                        |\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|{\"ttype\":\"tt\",\"userid\":\"1\",\"organizationid\":\"1\",\"userfirstname\":\"user1\",\"usermiddlename\":\"m1\",\"userlastname\":\"l1\",\"usertitle\":\"mr\",\"createdate\":\"1999-08-01\"}|\n|{\"ttype\":\"tt\",\"userid\":\"2\",\"organizationid\":\"2\",\"userfirstname\":\"user2\",\"usermiddlename\":\"m2\",\"userlastname\":\"l2\",\"usertitle\":\"mr\",\"createdate\":\"1999-09-01\"}|\n|{\"ttype\":\"tt\",\"userid\":\"3\",\"organizationid\":\"3\",\"userfirstname\":\"user3\",\"usermiddlename\":\"m3\",\"userlastname\":\"l3\",\"usertitle\":\"mr\",\"createdate\":\"1999-02-01\"}|\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1512426888832_-1196102009","id":"20171128-212413_501946510","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:571"},{"text":"val date1 = java.sql.Date.valueOf(\"2001-03-3\")\nval datepart = \"02/22/2017\".split(\"/\")\nval date2 = java.sql.Date.valueOf(Array(datepart(2),datepart(0),datepart(1)).mkString(\"-\"))\n\nval str = lit(\"2016-20-29\")\n\nval dateDF = spark.range(10)\n  .withColumn(\"today\", current_date())\n  .withColumn(\"now\", current_timestamp())\n\ndateDF.createOrReplaceTempView(\"dateTable\")\ndateDF.show(false)\n\n\ndateDF\n  .withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\n  .select(datediff(col(\"week_ago\"), col(\"today\")))\n  .show(1)\n\ndateDF\n  .select(\n    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n    to_date(lit(\"2017-05-22\")).alias(\"end\"))\n  .select(months_between(col(\"start\"), col(\"end\")))\n  .show(1)\n\n","dateUpdated":"2017-12-04T22:34:48+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512426888832_-1196102009","id":"20171127-015319_2096812207","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:572"},{"text":"//fileData.toDF.count()\n//fhlmc46m.count\n\n\n\nfiles.toDF.show\n\n//fileData.repartition(1).toDF.write.csv(\"gs://\" + projectId + \"/FHLMC/tmp/dataproc/\")\n\n//fhlmc46m.repartition(1).write.csv(\"gs://\" + projectId + \"/FHLMC/tmp/dataproc/\")","dateUpdated":"2017-12-04T22:34:48+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512426888833_-1196486758","id":"20171006-143419_914559290","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:573"},{"text":"//test.toDF().select(\"Age_Q\").show()\n//.select(\"Age_Q\").take(10).foreach(l => println(l))\nval fhlmc = test.toDF().registerTempTable(\"fhlmc\")\n\nval outputGcsPath = (\"gs://\" + projectId + \"/FHLMC/tmp/dataproc\")\n\n//val test_save = test.toDF().coalesce(1)\n//import org.apache.spark.sql.{SaveMode, SparkSession}\n//test_save.write.mode(SaveMode.Overwrite).csv(outputGcsPath + \"/hdfs/test_save.csv\")\n\n\ntest.select(\"Loan_num_q\").write.csv(\"gs://\" + projectId + \"/FHLMC/tmp/dataproc/test_ds_save.csv\")\n//test.saveAsTextFile(\"gs://\" + projectId + \"/FHLMC/tmp/dataproc\")\n\n\n","dateUpdated":"2017-12-04T22:34:48+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512426888833_-1196486758","id":"20170929-152315_65608423","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:574"},{"text":"%sql \nselect age_q, count(1) value\nfrom fhlmc \ngroup by age_q","dateUpdated":"2017-12-04T22:34:48+0000","config":{"editorSetting":{"language":"sql"},"colWidth":6,"editorMode":"ace/mode/sql","results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"stacked":false}},"commonSetting":{},"keys":[{"name":"age_q","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"value","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512426888834_-1195332512","id":"20170927-202907_282031011","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:575"},{"text":"%bigquery.sql\r\n#standardSQL\r\nSELECT loan_num_q, count(*) FROM `mortgage-data-warehouse.FHLMC.FHLMC_orig` \r\nwhere loan_num_q is null\r\ngroup by 1\r\n--limit 10","dateUpdated":"2017-12-04T22:34:48+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/sql","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512426888834_-1195332512","id":"20171005-143255_307464260","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:576"},{"text":"loanCounts: org.apache.spark.rdd.RDD[(String, Long)] = ShuffledRDD[56] at reduceByKey at <console>:71\r\nres99: Array[(String, Long)] = Array((1501052587,4500), (1501052581,4500), (1023599344,13000), (1023595551,12500), (1501052592,4500), (1501052590,4500), (1501052583,4500), (1501052585,4500), (1501052589,4500), (1023595687,12500), (1501052584,4500), (1501052580,4500), (1501052586,4500), (1501052588,4500), (1501052591,4500), (1501052593,4500), (1501052582,4500), (1023598090,13000))\r\n\r\n\r\nloanCounts: org.apache.spark.rdd.RDD[(String, Long)] = MapPartitionsRDD[61] at map at <console>:69\r\nres107: Array[(String, Long)] = Array((1023595687,12500), (1023598090,13000), (1023595551,12500), (1023599344,13000), (1501052590,4500), (1501052581,4500), (1501052583,4500), (1501052582,4500), (1501052592,4500), (1501052586,4500), (1501052593,4500), (1501052585,4500), (1501052584,4500), (1501052580,4500), (1501052591,4500), (1501052587,4500), (1501052588,4500), (1501052589,4500))\r\n","dateUpdated":"2017-12-04T22:34:48+0000","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512426888834_-1195332512","id":"20171006-191023_1063959915","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:577"},{"text":"val dfList = ('a' to 'd').map(col => (1 to 5).zip(col.toInt to col.toInt + 4).toDF(\"ID\", col.toString)).toList\ndfList","dateUpdated":"2017-12-04T22:34:48+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512426888834_-1195332512","id":"20171007-110851_2024412243","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:578"},{"text":"val t = (1 to 5).zip('a'.toInt to 'a'.toInt + 4).toDF(\"ID\", \"a\").toList ","dateUpdated":"2017-12-04T22:34:48+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512426888835_-1195717260","id":"20171007-120544_1946509542","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:579"},{"text":"","dateUpdated":"2017-12-04T22:34:48+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1512426888835_-1195717260","id":"20171120-200854_948827262","dateCreated":"2017-12-04T22:34:48+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:580"}],"name":"FHLMC_spark","id":"2CZ7T9SZC","angularObjects":{"2D31SZ3DQ:shared_process":[],"2D3JR5ZW9:shared_process":[],"2CZWKX39U:shared_process":[],"2CZSFG5GT:shared_process":[],"2D31DCKYV:shared_process":[],"2CZEHP5EM:shared_process":[],"2D1J29XB1:shared_process":[],"2D1XMZ6QX:shared_process":[],"2D1T9NS1C:shared_process":[],"2CZ1H1982:shared_process":[],"2D2SGN5D5:shared_process":[],"2D3DB7DBJ:shared_process":[],"2CYNUK3YR:shared_process":[],"2CZKZMAJ2:shared_process":[],"2D14B97D7:shared_process":[],"2D1VMNFDD:shared_process":[],"2D3AKPG7H:shared_process":[],"2D1RV8AD6:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}